{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![book](https://raw.githubusercontent.com/ageron/tensorflow-safari-course/master/images/intro_to_tf_course.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook accompanies my [Introduction to TensorFlow](https://www.safaribooksonline.com/live-training/courses/introduction-to-tensorflow/0636920079460/) live online training. It contains the code examples shown in the presentation, as well as the exercises and their solutions.\n",
    "\n",
    "**Try not to peek at the solutions when you go through the exercises. ;-)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's make sure this notebook works well in both Python 2 and Python 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> a = tf.constant(3)\n",
    ">>> b = tf.constant(5)\n",
    ">>> s = a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_1:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.framework.ops.Graph at 0x1152c7550>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> graph = tf.Graph()\n",
    ">>> with graph.as_default():\n",
    "...     a = tf.constant(3)\n",
    "...     b = tf.constant(5)\n",
    "...     s = a + b\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> with tf.Session(graph=graph) as sess:\n",
    "...     result = s.eval()\n",
    "...\n",
    ">>> result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> with tf.Session(graph=graph) as sess:\n",
    "...     result = sess.run(s)\n",
    "...\n",
    ">>> result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5, 8]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> with tf.Session(graph=graph) as sess:\n",
    "...     result = sess.run([a,b,s])\n",
    "...\n",
    ">>> result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1) Create a simple graph that calculates $ c = \\exp(\\sqrt 8 + 3) $.\n",
    "\n",
    "**Tip**: TensorFlow's API documentation is available at:\n",
    "https://www.tensorflow.org/versions/master/api_docs/python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    a = tf.exp(tf.sqrt(tf.constant(8.))+tf.constant(3.))\n",
    "    c = tf.exp(tf.sqrt(8.)+3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "...     result = sess.run([a,c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[339.82382, 339.82382]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2) Now create a `Session()` and evaluate the operation that gives you the result of the equation above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3) Create a graph that evaluates and prints both $ b = \\sqrt 8 $ and $ c = \\exp(\\sqrt 8 + 3) $. Try to implement this in a way that only evaluates $ \\sqrt 8 $ once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    b = tf.sqrt(8.)\n",
    "    c = tf.exp(b+3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "...     result = sess.run([b,c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.8284271, 339.82382]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.076768544898273&quot;).pbtxt = 'node {\\n  name: &quot;Variable/initial_value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable&quot;\\n  input: &quot;Variable/initial_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable_1/initial_value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable_1&quot;\\n  input: &quot;Variable_1/initial_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable_1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Variable/read&quot;\\n  input: &quot;Variable_1/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable&quot;\\n  input: &quot;add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truediv/y&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^Assign&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 2.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truediv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;Variable_1/read&quot;\\n  input: &quot;truediv/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Assign_1&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable_1&quot;\\n  input: &quot;truediv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable_1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^Variable/Assign&quot;\\n  input: &quot;^Variable_1/Assign&quot;\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.076768544898273&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4) The following code is needed to display TensorFlow graphs in Jupyter. Just run this cell then visualize your graph by calling `show_graph(`_your graph_`)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = b\"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def=None, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    graph_def = graph_def or tf.get_default_graph()\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try not to peek at the solution below before you have done the exercise! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![thinking](https://upload.wikimedia.org/wikipedia/commons/0/06/Filos_segundo_logo_%28flipped%29.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    c = tf.exp(tf.sqrt(tf.constant(8.)) + tf.constant(3.))\n",
    "    # or simply...\n",
    "    c = tf.exp(tf.sqrt(8.) + 3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph):\n",
    "    c_val = c.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    b = tf.sqrt(8.)\n",
    "    c = tf.exp(b + 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    b_val, c_val = sess.run([b, c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: the following implementation gives the right result, but it runs the graph twice, once to evaluate `b`, and once to evaluate `c`.  Since `c` depends on `b`, it means that `b` will be evaluated twice. Not what we wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WRONG!\n",
    "with tf.Session(graph=graph):\n",
    "    b_val = b.eval()  # evaluates b\n",
    "    c_val = c.eval()  # evaluates c, which means evaluating b again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> graph = tf.Graph()\n",
    ">>> with graph.as_default():\n",
    "...     x = tf.Variable(100)\n",
    "...     c = tf.constant(5)\n",
    "...     increment_op = tf.assign(x, x + c)\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> with tf.Session(graph=graph) as sess:\n",
    "...     x.initializer.run()\n",
    "...     print(x.eval())    # 100\n",
    "...     for iteration in range(10):\n",
    "...         increment_op.eval()\n",
    "...     print(x.eval())    # 150\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables Initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> graph = tf.Graph()\n",
    ">>> with graph.as_default():\n",
    "...     x = tf.Variable(100)\n",
    "...     c = tf.constant(5)\n",
    "...     increment_op = tf.assign(x, x + c)\n",
    "...     init = tf.global_variables_initializer()\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> with tf.Session(graph=graph) as sess:\n",
    "...     init.run()\n",
    "...     print(x.eval())    # 100\n",
    "...     for iteration in range(10):\n",
    "...         increment_op.eval()\n",
    "...     print(x.eval())    # 150\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> session1 = tf.Session(graph=graph)\n",
    ">>> session2 = tf.Session(graph=graph)\n",
    ">>> x.initializer.run(session=session1)\n",
    ">>> x.initializer.run(session=session2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> increment_op.eval(session=session1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> x.eval(session=session1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> x.eval(session=session2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> session1.close()\n",
    ">>> session2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will use TensorFlow to compute $ 1 + \\frac{1}{2} + \\frac{1}{4} + \\frac{1}{8} + \\cdots $ by creating a simple graph then running it multiple times.\n",
    "\n",
    "Think about how you would solve this problem (and if you are feeling confident enough, go ahead and implement your ideas), then follow the instructions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2.1) Create a graph with two variables $ x $ and $ y $, initialized to 0.0 and 1.0 respectively. Create an operation that will perform the following assignment: $ x \\gets x + y $. Create a second operation that will perform the following assignment: $ y \\gets \\dfrac{y}{2} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x = tf.Variable(0.0)\n",
    "    y = tf.Variable(1.0)\n",
    "    init = tf.global_variables_initializer()\n",
    "    first_op = tf.assign(x, x + y)\n",
    "    second_op = tf.assign(y, y/2.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2.2) Now create a `Session()`, initialize the variables, then create a loop that will run 50 times, and at each iteration will run the first assignment operation, then the second (separately). Finally, print out the value of $ x $. The result should be very close (or equal to) 2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    for iteration in range(50):\n",
    "        first_op.eval()\n",
    "        second_op.eval()\n",
    "    print(x.eval())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3) Try to run the assignment operations simultaneously. What happens to the result? Run your code multiply times: do the results vary? Can you explain what is happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.94586\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    for iteration in range(50):\n",
    "        sess.run([first_op, second_op])\n",
    "    print(x.eval())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5) Bonus question (if you have time): update you graph to define the second assignment ($y \\gets \\frac{y}{2}$) inside a `tf.control_dependencies()` block, to  guarantee that it runs after the first assignment ($ x \\gets x + y$). Does this finally solve the problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x = tf.Variable(0.0)\n",
    "    y = tf.Variable(1.0)\n",
    "    add = tf.assign(x, x + y)\n",
    "    with tf.control_dependencies([add]):\n",
    "        divide = tf.assign(y, y / 2)\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    for iteration in range(50):\n",
    "        sess.run([add, divide])\n",
    "    print(x.eval())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try not to peek at the solution below before you have done the exercise! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![thinking](https://upload.wikimedia.org/wikipedia/commons/0/06/Filos_segundo_logo_%28flipped%29.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x = tf.Variable(0.0)\n",
    "    y = tf.Variable(1.0)\n",
    "    add = tf.assign(x, x + y)\n",
    "    divide = tf.assign(y, y / 2)\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph):\n",
    "    init.run()\n",
    "    for iteration in range(20):\n",
    "        add.eval()\n",
    "        divide.eval()\n",
    "    result = x.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    for iteration in range(20):\n",
    "        sess.run([add, divide])\n",
    "    result = x.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x = tf.Variable(0.0)\n",
    "    y = tf.Variable(1.0)\n",
    "    add = tf.assign(x, x + y)\n",
    "    with tf.control_dependencies([add]):\n",
    "        divide = tf.assign(y, y / 2)\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    for iteration in range(30):\n",
    "        sess.run([add, divide])\n",
    "    result = x.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.ops.variables.Variable at 0x11539c7b8>,\n",
       " <tensorflow.python.ops.variables.Variable at 0x1152dddd8>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'variables'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.GraphKeys.GLOBAL_VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Exp:0' shape=() dtype=float32>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> graph.add_to_collection(\"my_collection\", c)\n",
    ">>> graph.get_collection(\"my_collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigating the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'Const' type=Const>,\n",
       " <tf.Operation 'Const_1' type=Const>,\n",
       " <tf.Operation 'add' type=Add>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> graph = tf.Graph()\n",
    ">>> with graph.as_default():\n",
    "...     a = tf.constant(3)\n",
    "...     b = tf.constant(5)\n",
    "...     s = a + b\n",
    "...\n",
    ">>> graph.get_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> graph.get_operation_by_name(\"add\") is s.op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> graph.get_tensor_by_name(\"add:0\") is s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Const:0' shape=() dtype=int32>,\n",
       " <tf.Tensor 'Const_1:0' shape=() dtype=int32>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> list(s.op.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'add:0' shape=() dtype=int32>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> list(s.op.outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naming Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> graph = tf.Graph()\n",
    ">>> with graph.as_default():\n",
    "...     a = tf.constant(3, name='a')\n",
    "...     b = tf.constant(5, name='b')\n",
    "...     s = tf.add(a, b, name='s')\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'a' type=Const>,\n",
       " <tf.Operation 'b' type=Const>,\n",
       " <tf.Operation 's' type=Add>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> graph.get_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "3.1) Create a graph with four variables named `\"x1\"`, `\"x2\"`, `\"x3\"` and `\"x4\"`, with initial values 1.0, 2.0, 3.0 and 4.0 respectively, then write some code that prints the name of every operation in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x1=tf.Variable(1.0)\n",
    "    x2=tf.Variable(2.0)\n",
    "    x3=tf.Variable(3.0)\n",
    "    x4=tf.Variable(4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'Variable/initial_value' type=Const>,\n",
       " <tf.Operation 'Variable' type=VariableV2>,\n",
       " <tf.Operation 'Variable/Assign' type=Assign>,\n",
       " <tf.Operation 'Variable/read' type=Identity>,\n",
       " <tf.Operation 'Variable_1/initial_value' type=Const>,\n",
       " <tf.Operation 'Variable_1' type=VariableV2>,\n",
       " <tf.Operation 'Variable_1/Assign' type=Assign>,\n",
       " <tf.Operation 'Variable_1/read' type=Identity>,\n",
       " <tf.Operation 'Variable_2/initial_value' type=Const>,\n",
       " <tf.Operation 'Variable_2' type=VariableV2>,\n",
       " <tf.Operation 'Variable_2/Assign' type=Assign>,\n",
       " <tf.Operation 'Variable_2/read' type=Identity>,\n",
       " <tf.Operation 'Variable_3/initial_value' type=Const>,\n",
       " <tf.Operation 'Variable_3' type=VariableV2>,\n",
       " <tf.Operation 'Variable_3/Assign' type=Assign>,\n",
       " <tf.Operation 'Variable_3/read' type=Identity>]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "3.2) Notice that for each `Variable`, TensorFlow actually created 4 operations:\n",
    "* the variable itself,\n",
    "* its initial value,\n",
    "* an assignment operation to assign the initial value to the variable,\n",
    "* and a read operation that you can safely ignore for now (for details, check out mrry's great answer to [this question](http://stackoverflow.com/questions/42783909/internals-of-variable-in-tensorflow)).\n",
    "\n",
    "Get the collection of global variables in the graph, and for each one of them use `get_operation_by_name()` to find its corresponding `/Assign` operation (just append `\"/Assign\"` to the variable's name).\n",
    "\n",
    "Hint: each object in the collection of global variables is actually a `Tensor`, not an `Operation` (it represents the variable's output, i.e., its value), so its name ends with `\":0\"`. You can get the `Operation` through the `Tensor`'s `op` attribute: its name will not end with `\":0\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.ops.variables.Variable at 0x1153db2e8>,\n",
       " <tensorflow.python.ops.variables.Variable at 0x1153db4a8>,\n",
       " <tensorflow.python.ops.variables.Variable at 0x1153db668>,\n",
       " <tensorflow.python.ops.variables.Variable at 0x1153db6a0>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'Variable/Assign' type=Assign>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_operation_by_name('Variable/Assign')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'Variable_1/Assign' type=Assign>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_operation_by_name('Variable_1/Assign')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'Variable_2/Assign' type=Assign>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_operation_by_name('Variable_2/Assign')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'Variable_3/Assign' type=Assign>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_operation_by_name('Variable_3/Assign')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3) Add a `tf.group()` to your graph, containing all the assignment operations you got in question 3.2. Congratulations! You have just reimplemented `tf.global_variables_initializer()`.\n",
    "\n",
    "Start a `Session()`, run your group operation, then evaluate each variable and print out the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph.add_to_collection(\"my_collection\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4) For each assignment operation you fetched earlier, get its second input and store it in a list. Next, start a session and evaluate that list (using `sess.run()`). Print out the result: you should see `[1.0, 2.0, 3.0, 4.0]`. Can you guess why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try not to peek at the solution below before you have done the exercise! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![thinking](https://upload.wikimedia.org/wikipedia/commons/0/06/Filos_segundo_logo_%28flipped%29.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x1 = tf.Variable(1.0, name=\"x1\")\n",
    "    x2 = tf.Variable(2.0, name=\"x2\")\n",
    "    x3 = tf.Variable(3.0, name=\"x3\")\n",
    "    x4 = tf.Variable(4.0, name=\"x4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gvars = graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "init_assign_ops = [graph.get_operation_by_name(gvar.op.name + \"/Assign\")\n",
    "                   for gvar in gvars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_assign_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    init = tf.group(*init_assign_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph):\n",
    "    init.run()\n",
    "    print(x1.eval())\n",
    "    print(x2.eval())\n",
    "    print(x3.eval())\n",
    "    print(x4.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_val_ops = [init_assign_op.inputs[1]\n",
    "                for init_assign_op in init_assign_ops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    print(sess.run(init_val_ops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: in the case of assignment operations, the first input is a reference to the variable, and the second is the assignment value. The assignment operations we have here are used to initialize the variables, so their assignment values correspond to the initial values: 1.0 for `x1`, 2.0 for `x2`, 3.0 for `x3` and 4.0 for `x4`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.loadtxt(\"data/life_satisfaction.csv\",\n",
    "                  dtype=np.float32,\n",
    "                  delimiter=\",\",\n",
    "                  skiprows=1,\n",
    "                  usecols=[1, 2])\n",
    "X_train = data[:, 0:1] / 10000 # feature scaling\n",
    "y_train = data[:, 1:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in `X_train` represents a training instance, in this case a country. In this simple regression example, there is just one feature per instance (i.e., one column), in this case the country's GDP per capita (in tens of thousands of dollars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.90549141],\n",
       "       [ 0.94373721],\n",
       "       [ 1.22398937],\n",
       "       [ 1.24953341],\n",
       "       [ 1.59917367],\n",
       "       [ 1.72880816],\n",
       "       [ 1.80642867],\n",
       "       [ 1.9121592 ],\n",
       "       [ 2.07324815],\n",
       "       [ 2.58647203],\n",
       "       [ 2.71951962],\n",
       "       [ 2.9866581 ],\n",
       "       [ 3.24855447],\n",
       "       [ 3.53433371],\n",
       "       [ 3.70448899],\n",
       "       [ 3.76750088],\n",
       "       [ 4.01066351],\n",
       "       [ 4.09965134],\n",
       "       [ 4.19739866],\n",
       "       [ 4.33319616],\n",
       "       [ 4.36031151],\n",
       "       [ 4.37240314],\n",
       "       [ 4.37706852],\n",
       "       [ 4.98662663],\n",
       "       [ 5.08545828],\n",
       "       [ 5.09618616],\n",
       "       [ 5.13507414],\n",
       "       [ 5.21141624],\n",
       "       [ 5.58052015]], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.        ],\n",
       "       [ 5.5999999 ],\n",
       "       [ 4.9000001 ],\n",
       "       [ 5.80000019],\n",
       "       [ 6.0999999 ],\n",
       "       [ 5.5999999 ],\n",
       "       [ 4.80000019],\n",
       "       [ 5.0999999 ],\n",
       "       [ 5.69999981],\n",
       "       [ 6.5       ],\n",
       "       [ 5.80000019],\n",
       "       [ 6.        ],\n",
       "       [ 5.9000001 ],\n",
       "       [ 7.4000001 ],\n",
       "       [ 7.30000019],\n",
       "       [ 6.5       ],\n",
       "       [ 6.9000001 ],\n",
       "       [ 7.        ],\n",
       "       [ 7.4000001 ],\n",
       "       [ 7.30000019],\n",
       "       [ 7.30000019],\n",
       "       [ 6.9000001 ],\n",
       "       [ 6.80000019],\n",
       "       [ 7.19999981],\n",
       "       [ 7.5       ],\n",
       "       [ 7.30000019],\n",
       "       [ 7.        ],\n",
       "       [ 7.5       ],\n",
       "       [ 7.19999981]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAncAAAFJCAYAAAD5UgsiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1164a0b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_life_satisfaction(X_train, y_train):\n",
    "    plt.plot(X_train * 10000, y_train, \"bo\")\n",
    "    plt.axis([0, 60000, 0, 10])\n",
    "    plt.xlabel(\"GDP per capita ($)\")\n",
    "    plt.ylabel(\"Life Satisfaction\")\n",
    "    plt.grid()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plot_life_satisfaction(X_train, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this exercise we will build a linear regression model using TensorFlow. If you are not familiar with the maths behind linear regression models, you can read the explanation below. If you already know this (or if you don't care much about the maths), you can just skip this explanation and simply follow the instructions given in questions 4.1 to 4.3 below.\n",
    "\n",
    "In a linear regression model, the predictions are a linear combination of the input features. In other words, the predicted value $\\hat{y}$ can be computed using the following equation:\n",
    "\n",
    "$\\hat{y} = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b$\n",
    "\n",
    "where:\n",
    "* $x_1, x_2, \\dots, x_n $ are the input features,\n",
    "* $w_1, w_2, \\dots, w_n $, are their corresponding weights,\n",
    "* and $b$ is the bias term (also called the intercept term).\n",
    "\n",
    "This equation can be expressed in a more compact way using vectors:\n",
    "\n",
    "$\\hat{y} = \\langle \\mathbf{x}, \\mathbf{w} \\rangle + b$\n",
    "\n",
    "where:\n",
    "* $ \\mathbf{x} = \\begin{pmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}$ is the input feature vector (by convention, vectors are written in bold font),\n",
    "* $ \\mathbf{w} = \\begin{pmatrix}w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\end{pmatrix}$ is the weight vector,\n",
    "* $\\langle \\mathbf{x}, \\mathbf{w} \\rangle$ is the inner product of vectors $\\mathbf{x}$ and $\\mathbf{w}$, equal to $w_1 x_1 + w_2 x_2 + \\dots + w_n x_n$.\n",
    "\n",
    "It is often more convenient to handle vectors as matrices with a single column (a \"column vector\"). The inner product $\\langle \\mathbf{x}, \\mathbf{w} \\rangle$ is then replaced with the matrix dot product: $\\mathbf{x}^T \\cdot \\mathbf{w}$, where $\\mathbf{x}^T$ is the transpose of the column vector $\\mathbf{x}$. Transposing a column vector gives you a \"row vector\" (i.e., a matrix with a single row): $\\mathbf{x}^T = \\begin{pmatrix} x_1 & x_2 & \\dots & x_n \\end{pmatrix}$. Once again $\\mathbf{x}^T \\cdot \\mathbf{w} = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n$.\n",
    "\n",
    "Lastly, it is possible to compute predictions for many instances at a time by putting all their input features in a matrix $\\mathbf{X}$ (by convention, matrices are in capital letters with a bold font, except when they just represent column or row vectors). The vector containing the predictions for every instance can be computed using the following equation:\n",
    "\n",
    "$\\hat{\\mathbf{y}} = \\mathbf{X} \\cdot \\mathbf{w} + b$\n",
    "\n",
    "where:\n",
    "* $ \\hat{\\mathbf{y}} = \\begin{pmatrix}\\hat{y}^{(1)} \\\\ \\hat{y}^{(2)} \\\\ \\vdots \\\\ \\hat{y}^{(m)} \\end{pmatrix}$ is the prediction vector, containing the predictions for all $m$ instances.\n",
    "* $ \\mathbf{X} = \\begin{pmatrix}x_1^{(1)} & x_2^{(1)} & \\cdots & x_n^{(1)} \\\\\n",
    "                                x_1^{(2)} & x_2^{(2)} & \\cdots & x_n^{(2)} \\\\\n",
    "                                \\vdots    & \\vdots    & \\ddots & \\vdots    \\\\\n",
    "                                x_1^{(m)} & x_2^{(m)} & \\cdots & x_n^{(m)}\\end{pmatrix} =                  \\begin{pmatrix}(\\mathbf{x}^{(1)})^T \\\\\n",
    "                                (\\mathbf{x}^{(2)})^T  \\\\\n",
    "                                \\vdots \\\\\n",
    "                                (\\mathbf{x}^{(m)})^T\\end{pmatrix} $ is the input feature matrix. It contains the input features of all instances for which you want to make predictions. Each row represents an instance, each column represents a feature.\n",
    "* Note that the matrix dot product $\\mathbf{X} \\cdot \\mathbf{w}$ returns a column vector, so when we add the bias term $b$, we mean adding that value to each and every element in the column vector (this is called _broadcasting_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "4.1) Create a graph containing:\n",
    "* a constant `X` initialized with `X_train`, which contains the input features of the training instances. In this particular example, there is just a single feature per instance (i.e., the GDP per capita).\n",
    "* a constant `y` initialized with `y_train`, which contains the labels of each instance (i.e., the life satisfaction).\n",
    "* a variable `b`, representing the bias term (initialized to 0.0).\n",
    "* a variable `w`, representing the weight vector (initialized to a column vector full of zeros, using `tf.zeros()`). Since there is just one input feature per instance in this example, this column vector contains a single row (it is a matrix with a single item).\n",
    "* an operation `y_pred` that computes the equation presented above: $\\hat{\\mathbf{y}} = \\mathbf{X} \\cdot \\mathbf{w} + b$. You will need to use `tf.matmul()`.\n",
    "* as always, don't forget to add an `init` operation, using `tf.global_variables_initializer()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    X = tf.constant(X_train, name=\"X\")\n",
    "    y = tf.constant(y_train, name=\"y\")\n",
    "\n",
    "    b = tf.Variable(0.0, name=\"b\")\n",
    "    w = tf.Variable(tf.zeros([1, 1]), name=\"w\")\n",
    "    y_pred = tf.add(tf.matmul(X, w), b, name=\"y_pred\")  # X @ w + b\n",
    "    \n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2) Start a session, run the `init` operation and evaluate the predictions `y_pred`. Since both variables `b` and `w` are initialized with zeros, you should get a vector full of zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    print(y_pred.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3) Let's measure how bad the model is using a cost function (also called a loss function). In regression tasks, it is common to use the Mean Square Error (MSE) as the cost function. It is given by the following equation:\n",
    "\n",
    "$\\text{MSE}(\\mathbf{w}, b) = \\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{(\\hat{y}^{(i)}-y^{(i)})^2}$.\n",
    "\n",
    "Add an `mse` operation to your graph, to compute the Mean Square Error. Hint: use `tf.reduce_mean()` and `tf.square()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4) Now start a session, initalize the variables and evaluate the MSE. As you can see, the result is quite high: this makes sense since we have not trained the model yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5) To find the optimal values for the model parameters (i.e., the variables `w` and `b`), we will use Gradient Descent. For this, we first need to compute the gradient of the cost function with regards to the model parameters.\n",
    "\n",
    "The gradient of the MSE with regards to the weight vector $\\mathbf{w}$ is:\n",
    "\n",
    "$\\nabla_{\\mathbf{w}}\\, \\text{MSE}(\\mathbf{w}, b) =\n",
    "\\begin{pmatrix}\n",
    " \\frac{\\partial}{\\partial w_0} \\text{MSE}(\\mathbf{w}, b) \\\\\n",
    " \\frac{\\partial}{\\partial w_1} \\text{MSE}(\\mathbf{w}, b) \\\\\n",
    " \\vdots \\\\\n",
    " \\frac{\\partial}{\\partial w_n} \\text{MSE}(\\mathbf{w}, b)\n",
    "\\end{pmatrix}\n",
    " = \\dfrac{2}{m} \\mathbf{X}^T \\cdot (\\hat{\\mathbf{y}} - \\mathbf{y})\n",
    "$\n",
    "\n",
    "And the partial derivative with regards to the bias $b$ is:\n",
    "\n",
    "$\n",
    "\\dfrac{\\partial}{\\partial b} \\text{MSE}(\\mathbf{w}, b) = \\dfrac{2}{m} \\sum\\limits_{i=1}^{m}(\\hat{y}^{(i)}-y^{(i)})\n",
    "$\n",
    "\n",
    "Add the operations `gradients_w` and `gradients_b` to your graph, using the equations above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.6) To perform a Gradient Descent step, we need to subtract the gradients (multiplied by the learning rate $\\eta$) from the weight vector and the bias:\n",
    "\n",
    "$\n",
    "\\mathbf{w} \\gets \\mathbf{w} - \\eta \\nabla_{\\mathbf{w}}\\, \\text{MSE}(\\mathbf{w}, b)\n",
    "$\n",
    "\n",
    "$\n",
    "\\mathbf{b} \\gets \\mathbf{b} - \\eta \\dfrac{\\partial}{\\partial b} \\text{MSE}(\\mathbf{w}, b)\n",
    "$\n",
    "\n",
    "Add two assignment operations, `tweak_w_ops` and `tweak_b_ops` that perform the assigments above, using a small learning rate $\\eta = 0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.7) That's it! We're ready to train the model. Start a session, initialize the variables, then write a loop that will repeatedly evaluate the assignment operations (e.g., 2000 times). Every 100 iterations, evaluate the MSE and print it out. Within a few hundred iterations the MSE should drop below 1.0, and eventually reach about 0.18. Congratulations! You built and trained your first Machine Learning model using TensorFlow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try not to peek at the solution below before you have done the exercise! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![thinking](https://upload.wikimedia.org/wikipedia/commons/0/06/Filos_segundo_logo_%28flipped%29.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    X = tf.constant(X_train, name=\"X\")\n",
    "    y = tf.constant(y_train, name=\"y\")\n",
    "\n",
    "    b = tf.Variable(0.0, name=\"b\")\n",
    "    w = tf.Variable(tf.zeros([1, 1]), name=\"w\")\n",
    "    y_pred = tf.add(tf.matmul(X, w), b, name=\"y_pred\")  # X @ w + b\n",
    "    \n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    print(y_pred.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    error = y_pred - y\n",
    "    square_error = tf.square(error)\n",
    "    mse = tf.reduce_mean(square_error, name=\"mse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    print(mse.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    m = len(X_train)\n",
    "    gradients_w = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "    gradients_b = 2 * tf.reduce_mean(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with graph.as_default():\n",
    "    tweak_w_op = tf.assign(w, w - learning_rate * gradients_w)\n",
    "    tweak_b_op = tf.assign(b, b - learning_rate * gradients_b)\n",
    "    training_op = tf.group(tweak_w_op, tweak_b_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 2000\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        if iteration % 100 == 0:\n",
    "            print(\"Iteration {:5}, MSE: {:.4f}\".format(iteration, mse.eval()))\n",
    "        training_op.run()\n",
    "    w_val, b_val = sess.run([w, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_life_satisfaction_with_linear_model(X_train, y_train, w, b):\n",
    "    plot_life_satisfaction(X_train, y_train)\n",
    "    plt.plot([0, 60000], [b, w[0][0] * (60000 / 10000) + b])\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plot_life_satisfaction_with_linear_model(X_train, y_train, w_val, b_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using autodiff Instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    X = tf.constant(X_train, dtype=tf.float32, name=\"X\")\n",
    "    y = tf.constant(y_train, dtype=tf.float32, name=\"y\")\n",
    "\n",
    "    b = tf.Variable(0.0, name=\"b\")\n",
    "    w = tf.Variable(tf.zeros([1, 1]), name=\"w\")\n",
    "    y_pred = tf.add(tf.matmul(X, w), b, name=\"y_pred\")  # X @ w + b\n",
    "    \n",
    "    mse = tf.reduce_mean(tf.square(y_pred - y), name=\"mse\")\n",
    "\n",
    "    gradients_w, gradients_b = tf.gradients(mse, [w, b])  # <= IT'S AUTODIFF MAGIC!\n",
    "\n",
    "    tweak_w_op = tf.assign(w, w - learning_rate * gradients_w)\n",
    "    tweak_b_op = tf.assign(b, b - learning_rate * gradients_b)\n",
    "    training_op = tf.group(tweak_w_op, tweak_b_op)\n",
    "\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 2000\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        if iteration % 100 == 0:\n",
    "            print(\"Iteration {:5}, MSE: {:.4f}\".format(iteration, mse.eval()))\n",
    "        training_op.run()\n",
    "    w_val, b_val = sess.run([w, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plot_life_satisfaction_with_linear_model(X_train, y_train, w_val, b_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Optimizers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    X = tf.constant(X_train, dtype=tf.float32, name=\"X\")\n",
    "    y = tf.constant(y_train, dtype=tf.float32, name=\"y\")\n",
    "\n",
    "    b = tf.Variable(0.0, name=\"b\")\n",
    "    w = tf.Variable(tf.zeros([1, 1]), name=\"w\")\n",
    "    y_pred = tf.add(tf.matmul(X, w), b, name=\"y_pred\")  # X @ w + b\n",
    "    \n",
    "    mse = tf.reduce_mean(tf.square(y_pred - y), name=\"mse\")\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(mse)  # <= MOAR AUTODIFF MAGIC!\n",
    "\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 2000\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        if iteration % 100 == 0:\n",
    "            print(\"Iteration {:5}, MSE: {:.4f}\".format(iteration, mse.eval()))\n",
    "        training_op.run()\n",
    "    w_val, b_val = sess.run([w, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plot_life_satisfaction_with_linear_model(X_train, y_train, w_val, b_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "momentum = 0.8\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    X = tf.constant(X_train, dtype=tf.float32, name=\"X\")\n",
    "    y = tf.constant(y_train, dtype=tf.float32, name=\"y\")\n",
    "\n",
    "    b = tf.Variable(0.0, name=\"b\")\n",
    "    w = tf.Variable(tf.zeros([1, 1]), name=\"w\")\n",
    "    y_pred = tf.add(tf.matmul(X, w), b, name=\"y_pred\")  # X @ w + b\n",
    "    \n",
    "    mse = tf.reduce_mean(tf.square(y_pred - y), name=\"mse\")\n",
    "\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(mse)\n",
    "\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 500\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        if iteration % 100 == 0:\n",
    "            print(\"Iteration {:5}, MSE: {:.4f}\".format(iteration, mse.eval()))\n",
    "        training_op.run()\n",
    "    w_val, b_val = sess.run([w, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plot_life_satisfaction_with_linear_model(X_train, y_train, w_val, b_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the optimizer know which variables to tweak? Answer: the `TRAINABLE_VARIABLES` collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll = graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "[var.op.name for var in coll]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions Outside of TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyprus_gdp_per_capita = 22000\n",
    "cyprus_life_satisfaction = w_val[0][0] * cyprus_gdp_per_capita / 10000 + b_val\n",
    "cyprus_life_satisfaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using placeholders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    X = tf.placeholder(tf.float32, shape=[None, 1], name=\"X\") # <= None allows for any\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 1], name=\"y\") #    training batch size\n",
    "\n",
    "    b = tf.Variable(0.0, name=\"b\")\n",
    "    w = tf.Variable(tf.zeros([1, 1]), name=\"w\")\n",
    "    y_pred = tf.add(tf.matmul(X, w), b, name=\"y_pred\")  # X @ w + b\n",
    "    \n",
    "    mse = tf.reduce_mean(tf.square(y_pred - y), name=\"mse\")\n",
    "\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(mse)\n",
    "\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 500\n",
    "\n",
    "X_test = np.array([[22000]], dtype=np.float32) / 10000\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        feed_dict = {X: X_train, y: y_train}\n",
    "        if iteration % 100 == 0:\n",
    "            print(\"Iteration {:5}, MSE: {:.4f}\".format(\n",
    "                iteration, \n",
    "                mse.eval(feed_dict))) # <= FEED TRAINING DATA\n",
    "        training_op.run(feed_dict)    # <= FEED TRAINING DATA\n",
    "    # make the prediction:\n",
    "    y_pred_val = y_pred.eval(feed_dict={X: X_test}) # <= FEED TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "5.1) Create a simple graph that computes the function $f(x) = x^2 - 3x + 1$. Define $x$ as a placeholder for a simple scalar value of type float32 value (i.e., `shape=[], dtype=tf.float32`). Create a session and evaluate $f(5)$. You should find 11.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2) Add an operation that computes the derivative of $f(x)$ with regards to $x$, noted $f'(x)$. Create a session and evaluate $f'(5)$. You should find 7.0.\n",
    "\n",
    "Hint: use `tf.gradients()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3) Using a `MomentumOptimizer`, find the value of $x$ that minimizes $f(x)$. You should find $\\hat{x}=1.5$.\n",
    "\n",
    "Hint: you need to change `x` into a `Variable`. Moreover, the `MomentumOptimizer` has its own variables that need to be initialized, so don't forget to create an `init` operation using a `tf.global_variables_initializer()`, and call it at the start of the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try not to peek at the solution below before you have done the exercise! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![thinking](https://upload.wikimedia.org/wikipedia/commons/0/06/Filos_segundo_logo_%28flipped%29.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 - Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=[], name=\"x\")\n",
    "    f = tf.square(x) - 3 * x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph):\n",
    "    print(f.eval(feed_dict={x: 5.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    [fp] = tf.gradients(f, [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph):\n",
    "    print(fp.eval(feed_dict={x: 5.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "momentum = 0.8\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x = tf.Variable(0.0, name=\"x\")\n",
    "    f = tf.square(x) - 3 * x + 1    \n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(f)\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 70\n",
    "with tf.Session(graph=graph):\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        training_op.run()\n",
    "        if iteration % 10 == 0:\n",
    "            print(\"x={:.2f}, f(x)={:.2f}\".format(x.eval(), f.eval()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it's possible to replace the output value of any operation, not just placeholders. So, for example, even though `x` is now a `Variable`, you can use a `feed_dict` to use any value you want, for example to compute `f(5.0)`. **Important**: this does _not_ affect the variable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph):\n",
    "    init.run()\n",
    "    print(x.eval()) # x == 0.0\n",
    "    print(f.eval()) # f(0) == 1.0\n",
    "    print(f.eval(feed_dict={x: 5.0})) # use 5.0 instead of the value of x, to compute f(5)\n",
    "    print(x.eval()) # x is still 0.0\n",
    "    print(f.eval()) # f(0) is still 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Restoring a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    X = tf.placeholder(tf.float32, shape=[None, 1], name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 1], name=\"y\")\n",
    "\n",
    "    b = tf.Variable(0.0, name=\"b\")\n",
    "    w = tf.Variable(tf.zeros([1, 1]), name=\"w\")\n",
    "    y_pred = tf.add(tf.matmul(X, w), b, name=\"y_pred\")  # X @ w + b\n",
    "    \n",
    "    mse = tf.reduce_mean(tf.square(y_pred - y), name=\"mse\")\n",
    "\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(mse)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver() # <= At the very end of the construction phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 500\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        if iteration % 100 == 0:\n",
    "            print(\"Iteration {:5}, MSE: {:.4f}\".format(\n",
    "                iteration, \n",
    "                mse.eval(feed_dict={X: X_train, y: y_train})))\n",
    "        training_op.run(feed_dict={X: X_train, y: y_train}) # <= FEED THE DICT\n",
    "    saver.save(sess, \"./my_life_satisfaction_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, \"./my_life_satisfaction_model\")\n",
    "    # make the prediction:\n",
    "    y_pred_val = y_pred.eval(feed_dict={X: X_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restoring a Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = \"./my_life_satisfaction_model\"\n",
    "graph = tf.Graph()\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # restore the graph\n",
    "    saver = tf.train.import_meta_graph(model_path + \".meta\")\n",
    "    saver.restore(sess, model_path)\n",
    "\n",
    "    # get references to the tensors we need\n",
    "    X = graph.get_tensor_by_name(\"X:0\")\n",
    "    y_pred = graph.get_tensor_by_name(\"y_pred:0\")\n",
    "\n",
    "    # make the prediction:\n",
    "    y_pred_val = y_pred.eval(feed_dict={X: X_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Readers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = [\"data/life_satisfaction.csv\"]\n",
    "n_epochs = 500\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    reader = tf.TextLineReader(skip_header_lines=1)\n",
    "\n",
    "    filename_queue = tf.train.string_input_producer(filenames, num_epochs=n_epochs)\n",
    "    record_id, record = reader.read(filename_queue)\n",
    "\n",
    "    record_defaults = [[''], [0.0], [0.0]]\n",
    "    country, gdp_per_capita, life_satisfaction = tf.decode_csv(record, record_defaults=record_defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "with graph.as_default():\n",
    "    X_batch, y_batch = tf.train.batch([gdp_per_capita, life_satisfaction], batch_size=batch_size)\n",
    "    X_batch_reshaped = tf.reshape(X_batch, [-1, 1])\n",
    "    y_batch_reshaped = tf.reshape(y_batch, [-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    X = tf.placeholder_with_default(X_batch_reshaped, shape=[None, 1], name=\"X\")\n",
    "    y = tf.placeholder_with_default(y_batch_reshaped, shape=[None, 1], name=\"y\")\n",
    "\n",
    "    b = tf.Variable(0.0, name=\"b\")\n",
    "    w = tf.Variable(tf.zeros([1, 1]), name=\"w\")\n",
    "    y_pred = tf.add(tf.matmul(X / 10000, w), b, name=\"y_pred\")  # X @ w + b\n",
    "    \n",
    "    mse = tf.reduce_mean(tf.square(y_pred - y), name=\"mse\")\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(mse, global_step=global_step)\n",
    "\n",
    "    init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    try:\n",
    "        while not coord.should_stop():\n",
    "            _, mse_val, global_step_val = sess.run([training_op, mse, global_step])\n",
    "            if global_step_val % 100 == 0:\n",
    "                print(global_step_val, mse_val)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"End of training\")\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    saver.save(sess, \"./my_life_satisfaction_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will learn to use TensorBoard. It is a great visualization tool that comes with TensorFlow. It works by parsing special TensorFlow logs, called _summaries_, and displaying them nicely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1) Starting the TensorBoard server. Open a Terminal and type the following commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move to the `tensorflow-safari-course` directory:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`~$` **`cd tensorflow-safari-course`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the `tf_logs` directory that will hold the TensorFlow data that we will want TensorBoard to display:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`~/tensorflow-safari-course$` **`mkdir tf_logs`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activate the virtual environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`~/tensorflow-safari-course$` **`source env/bin/activate`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the TensorBoard server:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(env) ~/tensorflow-safari-course$` **`tensorboard --logdir=tf_logs`**\n",
    "\n",
    "`Starting TensorBoard b'41' on port 6006\n",
    "(You can navigate to` http://127.0.1.1:6006 `)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now visit the URL given by TensorBoard. You should see the TensorBoard interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2) Now create a `tf.summary.FileWriter`, with the parameters: `logdir=\"tf_logs/run_number_1/\"` and `graph=graph` where `graph` is the one we built just before this exercise. This will automatically:\n",
    "* create the `run_number_1` directory inside the `tf_logs` directory,\n",
    "* create an `events.out.tfevents.*` file in that subdirectory that will contain the data that TensorBoard will display,\n",
    "* write the graph's definition to this file.\n",
    "\n",
    "Next, try refreshing the TensorBoard page in your browser (you may need to wait a couple minutes for it to detect the change, or else you can just restart the TensorBoard server). Visit the Graph tab: you should be able to visualize the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.3) As you can see, the graph looks really messy in TensorBoard. We need to organize it a bit. For this, name scopes come in handy. An operation can be placed inside a name scope in one of two ways:\n",
    "\n",
    "* Add the scope as a prefix to the operation's name, for example:\n",
    "\n",
    "```python\n",
    "a = tf.constant(0.0, name=\"my_name_scope/a\")\n",
    "```\n",
    "\n",
    "* Or (generally clearer) use a `tf.name_scope()` block, for example:\n",
    "\n",
    "```python\n",
    "with tf.name_scope(\"my_name_scope\"):\n",
    "    a = tf.constant(0.0, name=\"a\")\n",
    "```\n",
    "\n",
    "Add name scopes to the following graph, then write it to TensorBoard (using a different run number for the log directory name) and see how much better it looks, and how much easier it is to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = [\"data/life_satisfaction.csv\"]\n",
    "n_epochs = 500\n",
    "batch_size = 5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    reader = tf.TextLineReader(skip_header_lines=1)\n",
    "\n",
    "    filename_queue = tf.train.string_input_producer(filenames, num_epochs=n_epochs)\n",
    "    record_id, record = reader.read(filename_queue)\n",
    "\n",
    "    record_defaults = [[''], [0.0], [0.0]]\n",
    "    country, gdp_per_capita, life_satisfaction = tf.decode_csv(record, record_defaults=record_defaults)\n",
    "\n",
    "    X_batch, y_batch = tf.train.batch([gdp_per_capita, life_satisfaction], batch_size=batch_size)\n",
    "    X_batch_reshaped = tf.reshape(X_batch, [-1, 1])\n",
    "    y_batch_reshaped = tf.reshape(y_batch, [-1, 1])\n",
    "\n",
    "    X = tf.placeholder_with_default(X_batch_reshaped, shape=[None, 1], name=\"X\")\n",
    "    y = tf.placeholder_with_default(y_batch_reshaped, shape=[None, 1], name=\"y\")\n",
    "\n",
    "    b = tf.Variable(0.0, name=\"b\")\n",
    "    w = tf.Variable(tf.zeros([1, 1]), name=\"w\")\n",
    "    y_pred = tf.add(tf.matmul(X / 10000, w), b, name=\"y_pred\")  # X @ w + b\n",
    "    \n",
    "    mse = tf.reduce_mean(tf.square(y_pred - y), name=\"mse\")\n",
    "    global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(mse, global_step=global_step)\n",
    "        \n",
    "    init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.4) Print out the name of a few operations. Notice how the names now have the scope as a prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.5) TensorBoard is capable of displaying data from multiple TensorFlow runs (for example multiple training sessions). For this, we need to place the data from each run in a different subdirectory of the `tf_logs` directory. We can name these subdirectories however we want, but a simple option is to name them using a timestamp. The following `logdir()` function returns the path of a subdirectory whose name is based on the current date and time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def logdir():\n",
    "    root_logdir = \"tf_logs\"\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    return \"{}/run_{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a few different graphs and instantiate a different FileWriter for each one, using a different log directory every time (with the help of the `logdir()` function). Refresh TensorBoard and notice that you can browse any graph you want by selecting the appropriate run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.6) Now we will use TensorBoard to visualize the learning curve, that is the evolution of the cost function during training.\n",
    "\n",
    "* First add a scalar summary operation in the graph, using `tf.summary.scalar(\"MSE\", mse)`.\n",
    "* Next, update the training code to evaluate this scalar summary and write the result to the events file using the `FileWriter`'s `add_summary()` method (also specifying the training step). For performance reasons, you probably want to do this only every 10 training iterations or so.\n",
    "* Next, train the model.\n",
    "* Refresh TensorBoard, and visit the Scalars tab. Select the appropriate run and visualize the learning curve. Try zooming in and out, and play around with the options, in particular the smoothing option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try not to peek at the solution below before you have done the exercise! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![thinking](https://upload.wikimedia.org/wikipedia/commons/0/06/Filos_segundo_logo_%28flipped%29.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6 - Solution\n",
    "6.1)\n",
    "\n",
    "N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.FileWriter(\"tf_logs/run_number_1_solution/\", graph=graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = [\"data/life_satisfaction.csv\"]\n",
    "n_epochs = 500\n",
    "batch_size = 5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.name_scope(\"reader\"):\n",
    "        reader = tf.TextLineReader(skip_header_lines=1)\n",
    "\n",
    "        filename_queue = tf.train.string_input_producer(filenames, num_epochs=n_epochs)\n",
    "        record_id, record = reader.read(filename_queue)\n",
    "\n",
    "        record_defaults = [[''], [0.0], [0.0]]\n",
    "        country, gdp_per_capita, life_satisfaction = tf.decode_csv(record, record_defaults=record_defaults)\n",
    "\n",
    "        X_batch, y_batch = tf.train.batch([gdp_per_capita, life_satisfaction], batch_size=batch_size)\n",
    "        X_batch_reshaped = tf.reshape(X_batch, [-1, 1])\n",
    "        y_batch_reshaped = tf.reshape(y_batch, [-1, 1])\n",
    "\n",
    "    with tf.name_scope(\"linear_model\"):\n",
    "        X = tf.placeholder_with_default(X_batch_reshaped, shape=[None, 1], name=\"X\")\n",
    "        y = tf.placeholder_with_default(y_batch_reshaped, shape=[None, 1], name=\"y\")\n",
    "\n",
    "        b = tf.Variable(0.0, name=\"b\")\n",
    "        w = tf.Variable(tf.zeros([1, 1]), name=\"w\")\n",
    "        y_pred = tf.add(tf.matmul(X / 10000, w), b, name=\"y_pred\")  # X @ w + b\n",
    "    \n",
    "    with tf.name_scope(\"train\"):\n",
    "        mse = tf.reduce_mean(tf.square(y_pred - y), name=\"mse\")\n",
    "        global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "        optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "        training_op = optimizer.minimize(mse, global_step=global_step)\n",
    "        \n",
    "    init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.FileWriter(\"tf_logs/run_number_2_solution/\", graph=graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country.name, gdp_per_capita.name, X_batch.name, y_batch.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.name, y.name, b.name, w.name, y_pred.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse.name, global_step.name, training_op.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph1 = tf.Graph()\n",
    "with graph1.as_default():\n",
    "    a = tf.constant(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.FileWriter(logdir(), graph=graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph2 = tf.Graph()\n",
    "with graph2.as_default():\n",
    "    a = tf.constant(1.0, name=\"a\")\n",
    "    b = tf.Variable(2.0, name=\"b\")\n",
    "    c = a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run `logdir()` twice within the same second, we will get the same directory name twice. To avoid this, let's wait a bit over 1 second here. In real life, this is quite unlikely to happen since training a model typically takes much longer than 1 second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.FileWriter(logdir(), graph=graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time.sleep(1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = [\"data/life_satisfaction.csv\"]\n",
    "n_epochs = 500\n",
    "batch_size = 5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.name_scope(\"reader\"):\n",
    "        reader = tf.TextLineReader(skip_header_lines=1)\n",
    "\n",
    "        filename_queue = tf.train.string_input_producer(filenames, num_epochs=n_epochs)\n",
    "        record_id, record = reader.read(filename_queue)\n",
    "\n",
    "        record_defaults = [[''], [0.0], [0.0]]\n",
    "        country, gdp_per_capita, life_satisfaction = tf.decode_csv(record, record_defaults=record_defaults)\n",
    "\n",
    "        X_batch, y_batch = tf.train.batch([gdp_per_capita, life_satisfaction], batch_size=batch_size)\n",
    "        X_batch_reshaped = tf.reshape(X_batch, [-1, 1])\n",
    "        y_batch_reshaped = tf.reshape(y_batch, [-1, 1])\n",
    "\n",
    "    with tf.name_scope(\"linear_model\"):\n",
    "        X = tf.placeholder_with_default(X_batch_reshaped, shape=[None, 1], name=\"X\")\n",
    "        y = tf.placeholder_with_default(y_batch_reshaped, shape=[None, 1], name=\"y\")\n",
    "\n",
    "        b = tf.Variable(0.0, name=\"b\")\n",
    "        w = tf.Variable(tf.zeros([1, 1]), name=\"w\")\n",
    "        y_pred = tf.add(tf.matmul(X / 10000, w), b, name=\"y_pred\")\n",
    "    \n",
    "    with tf.name_scope(\"train\"):\n",
    "        mse = tf.reduce_mean(tf.square(y_pred - y), name=\"mse\")\n",
    "        mse_summary = tf.summary.scalar('MSE', mse)                     # <= ADDED\n",
    "        global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "        optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "        training_op = optimizer.minimize(mse, global_step=global_step)\n",
    "        \n",
    "    init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.FileWriter(logdir(), graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    try:\n",
    "        while not coord.should_stop():\n",
    "            _, mse_summary_val, global_step_val = sess.run([training_op, mse_summary, global_step])\n",
    "            if global_step_val % 10 == 0:\n",
    "                summary_writer.add_summary(mse_summary_val, global_step_val)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"End of training\")\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    saver.save(sess, \"./my_life_satisfaction_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visit the [TensorFlow Playground](http://playground.tensorflow.org).\n",
    "* Try training the default neural network by clicking the \"Run\" button (top left). Notice how it quickly finds a good solution for the classification task. Notice that the neurons in the first hidden layer have learned simple patterns, while the neurons in the second hidden layer have learned to combine the simple patterns of the first hidden layer into more complex patterns). In general, the more layers, the more complex the patterns can be.\n",
    "* Try replacing the Tanh activation function with the ReLU activation function, and train the network again. Notice that it finds a solution even faster, but this time the boundaries are linear. This is due to the shape of the ReLU function.\n",
    "* Modify the network architecture to have just one hidden layer with three neurons. Train it multiple times (to reset the network weights, just add and remove a neuron). Notice that the training time varies a lot, and sometimes it even gets stuck in a local minimum.\n",
    "* Now remove one neuron to keep just 2. Notice that the neural network is now incapable of finding a good solution, even if you try multiple times. The model has too few parameters and it systematically underfits the training set.\n",
    "* Next, set the number of neurons to 8 and train the network several times. Notice that it is now consistently fast and never gets stuck. This highlights an important finding in neural network theory: large neural networks almost never get stuck in local minima, and even when they do these local optima are almost as good as the global optimum. However, they can still get stuck on long plateaus for a long time.\n",
    "* Now change the dataset to be the spiral (bottom right dataset under \"DATA\"). Change the network architecture to have 4 hidden layers with 8 neurons each. Notice that training takes much longer, and often gets stuck on plateaus for long periods of time. Also notice that the neurons in the highest layers (i.e. on the right) tend to evolve faster than the neurons in the lowest layers (i.e. on the left). This problem, called the \"vanishing gradients\" problem, can be alleviated using better weight initialization and other techniques, better optimizers (such as AdaGrad or Adam), or using Batch Normalization.\n",
    "* Go ahead and play with the other parameters to get a feel of what they do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "X_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_data in X_batch:\n",
    "    plt.imshow(image_data.reshape([28, 28]), cmap=\"binary\", interpolation=\"nearest\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.1) Take a close look at the following neural network model and make sure you understand every line. Next, add an extra hidden layer composed of 100 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.name_scope(\"inputs\"):\n",
    "        X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "        y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "\n",
    "    with tf.name_scope(\"hidden1\"):\n",
    "        b1 = tf.Variable(tf.zeros([n_hidden1]), name=\"b1\")\n",
    "        W1 = tf.Variable(tf.random_uniform([n_inputs, n_hidden1], -1.0, 1.0, seed=42), name=\"W1\")\n",
    "        hidden1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    \n",
    "    with tf.name_scope(\"output\"):\n",
    "        b2 = tf.Variable(tf.zeros([n_outputs]), name=\"b2\")\n",
    "        W2 = tf.Variable(tf.random_uniform([n_hidden1, n_outputs], -1.0, 1.0, seed=42), name=\"W2\")\n",
    "        logits = tf.matmul(hidden1, W2) + b2\n",
    "        Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "    \n",
    "    with tf.name_scope(\"train\"):\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "        loss = tf.reduce_mean(xentropy)\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    with tf.name_scope(\"init_and_save\"):\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.2) Write the training code, and train the model for about 20 epochs (i.e. enough training iterations to go through the training set 20 times). Evaluate it on the test set: you should get over 95% accuracy.\n",
    "\n",
    "Hint: you should open a session, initialize the variables, then write the main training loop. Inside it you should use `minst.train.next_batch(batch_size)` to get the next training batch (say with `batch_size=50`), then run the `training_op`, feeding it the training batch (don't forget to feed both `X` and `y`). Every few hundred iterations, evaluate the model's accuracy on the test set (`mnist.test.images` and `mnist.test.labels`), and print the result. At the end of training, save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.3) Bonus question: load the model you just trained and saved, and use it to make predictions on the first 200 images of the test set. Display the images that the model got wrong, and show the class probabilities that it guessed. Notice that some of the images it gets wrong are pretty poorly written, but some are obvious to us humans. We will see that Convolutional Neural Networks can do a much better job and reach human performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try not to peek at the solution below before you have done the exercise! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![thinking](https://upload.wikimedia.org/wikipedia/commons/0/06/Filos_segundo_logo_%28flipped%29.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8 - Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.name_scope(\"inputs\"):\n",
    "        X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "        y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "\n",
    "    with tf.name_scope(\"hidden1\"):\n",
    "        b1 = tf.Variable(tf.zeros([n_hidden1]), name=\"b1\")\n",
    "        W1 = tf.Variable(tf.random_uniform([n_inputs, n_hidden1], -1.0, 1.0, seed=42), name=\"W1\")\n",
    "        hidden1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "    with tf.name_scope(\"hidden2\"):\n",
    "        b2 = tf.Variable(tf.zeros([n_hidden2]), name=\"b2\")\n",
    "        W2 = tf.Variable(tf.random_uniform([n_hidden1, n_hidden2], -1.0, 1.0, seed=42), name=\"W2\")\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, W2) + b2)\n",
    "\n",
    "    with tf.name_scope(\"output\"):\n",
    "        b3 = tf.Variable(tf.zeros([n_outputs]), name=\"b3\")\n",
    "        W3 = tf.Variable(tf.random_uniform([n_hidden2, n_outputs], -1.0, 1.0, seed=42), name=\"W3\")\n",
    "        logits = tf.matmul(hidden2, W3) + b3\n",
    "        Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "    \n",
    "    with tf.name_scope(\"train\"):\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "        loss = tf.reduce_mean(xentropy)\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    with tf.name_scope(\"init_and_save\"):\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_mnist_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver = tf.train.import_meta_graph(\"./my_mnist_model.meta\")\n",
    "    saver.restore(sess, \"./my_mnist_model\")\n",
    "    X = graph.get_tensor_by_name(\"inputs/X:0\")\n",
    "    Y_proba = graph.get_tensor_by_name(\"output/Y_proba:0\")\n",
    "    Y_proba_val = Y_proba.eval(feed_dict={X: mnist.test.images})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example_index in range(200):\n",
    "    y_proba = Y_proba_val[example_index]\n",
    "    y_pred = np.argmax(y_proba)\n",
    "    y_label = mnist.test.labels[example_index]\n",
    "    if y_pred != y_label:\n",
    "        print(\"Actual class:{}, Predicted class: {}, Main probabilities: {}\".format(\n",
    "                  y_label,\n",
    "                  y_pred,\n",
    "                  \", \".join([\"{}:{:.1f}%\".format(n, 100*p)\n",
    "                             for n, p in enumerate(y_proba) if p > 0.01])))\n",
    "        plt.imshow(mnist.test.images[example_index].reshape([28, 28]), cmap=\"binary\", interpolation=\"nearest\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Organizing Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_net_layer(inputs, n_neurons, activation=None, seed=None):\n",
    "    n_inputs = int(inputs.get_shape()[1])\n",
    "    b = tf.Variable(tf.zeros([n_neurons]), name=\"b\")\n",
    "    W = tf.Variable(tf.random_uniform([n_inputs, n_neurons], -1.0, 1.0, seed=seed), name=\"W\")\n",
    "    logits = tf.matmul(inputs, W) + b\n",
    "    if activation:\n",
    "        return activation(logits)\n",
    "    else:\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simplify our code by using `neural_net_layer()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.name_scope(\"inputs\"):\n",
    "        X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "        y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "\n",
    "#########################################################################\n",
    "# This section is simplified (the rest is unchanged)\n",
    "#\n",
    "    with tf.name_scope(\"hidden1\"):\n",
    "        hidden1 = neural_net_layer(X, n_hidden1, activation=tf.nn.relu) # <= CHANGED\n",
    "\n",
    "    with tf.name_scope(\"output\"):\n",
    "        logits = neural_net_layer(hidden1, n_outputs)                   # <= CHANGED\n",
    "        Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "#\n",
    "#\n",
    "#########################################################################\n",
    "    \n",
    "    with tf.name_scope(\"train\"):\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "        loss = tf.reduce_mean(xentropy)\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    with tf.name_scope(\"init_and_save\"):\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[var.op.name for var in graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that training still works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_mnist_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use `tf.layers.dense()` instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.name_scope(\"inputs\"):\n",
    "        X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "        y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "\n",
    "    with tf.name_scope(\"hidden1\"):\n",
    "        hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\") # <= CHANGED\n",
    "\n",
    "    with tf.name_scope(\"output\"):\n",
    "        logits = tf.layers.dense(hidden1, n_outputs, name=\"output\")                    # <= CHANGED\n",
    "        Y_proba = tf.nn.softmax(logits)\n",
    "    \n",
    "    with tf.name_scope(\"train\"):\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "        loss = tf.reduce_mean(xentropy)\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    with tf.name_scope(\"init_and_save\"):\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[var.op.name for var in graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that training still works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_mnist_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose you want two more hidden layers with shared weights & biases. Let's use variable scopes for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28\n",
    "n_hidden = 100\n",
    "n_outputs = 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.name_scope(\"inputs\"):\n",
    "        X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "        y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "\n",
    "    hidden1 = tf.layers.dense(X, n_hidden, activation=tf.nn.relu, name=\"hidden1\")                    # <= CHANGED\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden, activation=tf.nn.relu, name=\"hidden23\")             # <= CHANGED\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden, activation=tf.nn.relu, name=\"hidden23\", reuse=True) # <= CHANGED\n",
    "\n",
    "    with tf.name_scope(\"output\"):\n",
    "        logits = tf.layers.dense(hidden3, n_outputs, name=\"output\")\n",
    "        Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "    \n",
    "    with tf.name_scope(\"train\"):\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "        loss = tf.reduce_mean(xentropy)\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    with tf.name_scope(\"init_and_save\"):\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[var.op.name for var in graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that training works well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_mnist_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would we implement variable sharing in `neural_net_layer()`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_net_layer(inputs, n_neurons, activation=None, name=None, reuse=None, seed=None):\n",
    "    with tf.variable_scope(name, default_name=\"layer\", reuse=reuse):\n",
    "        n_inputs = int(inputs.get_shape()[1])\n",
    "        rnd_init = lambda shape, dtype, partition_info: tf.random_uniform(shape, -1.0, 1.0, dtype=dtype, seed=seed)\n",
    "        b = tf.get_variable(\"biases\", shape=[n_neurons], initializer=rnd_init)\n",
    "        W = tf.get_variable(\"weights\", shape=[n_inputs, n_neurons], initializer=rnd_init)\n",
    "        logits = tf.matmul(inputs, W) + b\n",
    "        if activation:\n",
    "            return activation(logits)\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.variable_scope(\"foo\"): \n",
    "        a = tf.constant(1., name=\"a\")\n",
    "        with tf.name_scope(\"bar\"): \n",
    "            b = tf.constant(2., name=\"b\")\n",
    "            with tf.name_scope(\"baz\"):\n",
    "                c = tf.get_variable(\"c\", shape=[], initializer=tf.constant_initializer(2))\n",
    "                s = tf.add_n([a,b,c], name=\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Techniques for Training Deep Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using He initialization and the ELU activation function (with the help of a `partial()`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.name_scope(\"inputs\"):\n",
    "        X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "        y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    dense_layer = partial(tf.layers.dense,\n",
    "                          kernel_initializer=he_init,\n",
    "                          activation=tf.nn.elu)\n",
    "    hidden1 = dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    hidden2 = dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
    "    logits = dense_layer(hidden2, n_outputs, activation=None, name=\"output\")\n",
    "    Y_proba = tf.nn.softmax(logits)\n",
    "    \n",
    "    with tf.name_scope(\"train\"):\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "        loss = tf.reduce_mean(xentropy)\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    with tf.name_scope(\"init_and_save\"):\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_mnist_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will add a 50% dropout rate to the following neural network model below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.1) Add a `training` placeholder, of type `tf.bool`.\n",
    "\n",
    "Tip: you can use `tf.placeholder_with_default()` to make this `False` by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.2) Add a dropout layer between the input layer and the first hidden layer, using `tf.layers.dropout()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.name_scope(\"inputs\"):\n",
    "        X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "        y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "    dense_layer = partial(tf.layers.dense,\n",
    "                          kernel_initializer=he_init,\n",
    "                          activation=tf.nn.elu)\n",
    "    hidden1 = dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    hidden2 = dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
    "    logits = dense_layer(hidden2, n_outputs, activation=None, name=\"output\")\n",
    "    Y_proba = tf.nn.softmax(logits)\n",
    "    \n",
    "    with tf.name_scope(\"train\"):\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "        loss = tf.reduce_mean(xentropy)\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    with tf.name_scope(\"init_and_save\"):\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.3) Update the following training code to feed the value of the `training` placeholder, where appropriate, then run the code and see if the model performs better than without dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_mnist_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try not to peek at the solution below before you have done the exercise! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![thinking](https://upload.wikimedia.org/wikipedia/commons/0/06/Filos_segundo_logo_%28flipped%29.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9 - Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.1-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "dropout_rate = 0.5                                                               # <= CHANGED\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.name_scope(\"inputs\"):\n",
    "        X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "        y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "        training = tf.placeholder_with_default(False, shape=[], name='training') # <= CHANGED\n",
    "        X_drop = tf.layers.dropout(X, dropout_rate, training=training)           # <= CHANGED\n",
    "\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "    dense_layer = partial(tf.layers.dense,\n",
    "                          kernel_initializer=he_init,\n",
    "                          activation=tf.nn.elu)\n",
    "    hidden1 = dense_layer(X_drop, n_hidden1, name=\"hidden1\")                     # <= CHANGED\n",
    "    hidden2 = dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
    "    logits = dense_layer(hidden2, n_outputs, activation=None, name=\"output\")\n",
    "    Y_proba = tf.nn.softmax(logits)\n",
    "    \n",
    "    with tf.name_scope(\"train\"):\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "        loss = tf.reduce_mean(xentropy)\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    with tf.name_scope(\"init_and_save\"):\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True}) # <= CHANGED\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_mnist_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 50\n",
    "\n",
    "best_acc_val = 0\n",
    "check_interval = 100\n",
    "checks_since_last_progress = 0\n",
    "max_checks_without_progress = 100\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
    "            if iteration % check_interval == 0:\n",
    "                acc_val = accuracy.eval(feed_dict={X: mnist.test.images[:2000], y: mnist.test.labels[:2000]})\n",
    "                if acc_val > best_acc_val:\n",
    "                    best_acc_val = acc_val\n",
    "                    checks_since_last_progress = 0\n",
    "                    saver.save(sess, \"./my_best_model_so_far\")\n",
    "                else:\n",
    "                    checks_since_last_progress += 1                    \n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images[2000:], y: mnist.test.labels[2000:]})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test, \"Best validation accuracy:\", best_acc_val)\n",
    "        if checks_since_last_progress > max_checks_without_progress:\n",
    "            print(\"Early stopping!\")\n",
    "            saver.restore(sess, \"./my_best_model_so_far\")\n",
    "            break\n",
    "\n",
    "    acc_test = accuracy.eval(feed_dict={X: mnist.test.images[2000:], y: mnist.test.labels[2000:]})\n",
    "    print(\"Final accuracy on test set:\", acc_test)\n",
    "    save_path = saver.save(sess, \"./my_mnist_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model to disk so often slows down training. Let's save to RAM instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_params():\n",
    "    gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "    return {gvar.op.name: value for gvar, value in zip(gvars, tf.get_default_session().run(gvars))}\n",
    "\n",
    "def restore_model_params(model_params):\n",
    "    gvar_names = list(model_params.keys())\n",
    "    assign_ops = {gvar_name: tf.get_default_graph().get_operation_by_name(gvar_name + \"/Assign\")\n",
    "                  for gvar_name in gvar_names}\n",
    "    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "    tf.get_default_session().run(assign_ops, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 50\n",
    "\n",
    "best_acc_val = 0\n",
    "check_interval = 100\n",
    "checks_since_last_progress = 0\n",
    "max_checks_without_progress = 100\n",
    "best_model_params = None\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
    "            if iteration % check_interval == 0:\n",
    "                acc_val = accuracy.eval(feed_dict={X: mnist.test.images[:2000], y: mnist.test.labels[:2000]})\n",
    "                if acc_val > best_acc_val:\n",
    "                    best_acc_val = acc_val\n",
    "                    checks_since_last_progress = 0\n",
    "                    best_model_params = get_model_params()\n",
    "                else:\n",
    "                    checks_since_last_progress += 1\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images[2000:], y: mnist.test.labels[2000:]})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test, \"Best validation accuracy:\", best_acc_val)\n",
    "        if checks_since_last_progress > max_checks_without_progress:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "    if best_model_params:\n",
    "        restore_model_params(best_model_params)\n",
    "    acc_test = accuracy.eval(feed_dict={X: mnist.test.images[2000:], y: mnist.test.labels[2000:]})\n",
    "    print(\"Final accuracy on test set:\", acc_test)\n",
    "    save_path = saver.save(sess, \"./my_mnist_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load demo image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.misc import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "china = imread(\"./images/china.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "china.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_image(image):\n",
    "    cmap = \"gray\" if len(image.shape) == 2 else None\n",
    "    plt.imshow(image, cmap=cmap, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plot_image(china)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crop it and convert it to grayscale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = china[150:220, 130:250].mean(axis=2).astype(np.float32)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "height, width = image.shape\n",
    "channels = 1  # grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plot_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_filters = np.zeros(shape=(7, 7, 1, 2), dtype=np.float32)  # height, width, in channels, out channels\n",
    "basic_filters[:, 3, 0, 0] = 1\n",
    "basic_filters[3, :, 0, 1] = 1\n",
    "plot_image(basic_filters[:, :, 0, 0])\n",
    "plt.show()\n",
    "plot_image(basic_filters[:, :, 0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    X = tf.placeholder(tf.float32, shape=(None, height, width, channels))\n",
    "    filters = tf.constant(basic_filters)\n",
    "    convolution = tf.nn.conv2d(X, filters, strides=[1,1,1,1], padding=\"SAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    X_batch = image.reshape(1, height, width, 1)\n",
    "    output = convolution.eval(feed_dict={X: X_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plot_image(output[0, :, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plot_image(output[0, :, :, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add a max pooling layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    X = tf.placeholder(tf.float32, shape=(None, height, width, channels))\n",
    "    filters = tf.constant(basic_filters)\n",
    "    convolution = tf.nn.conv2d(X, filters, strides=[1,1,1,1], padding=\"SAME\")\n",
    "    max_pool = tf.nn.max_pool(convolution, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    X_batch = image.reshape(1, height, width, 1)\n",
    "    output = max_pool.eval(feed_dict={X: X_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plot_image(output[0, :, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plot_image(output[0, :, :, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final exercise, you will tackle MNIST and reach over 99% accuracy using most of what you learned in this course:\n",
    "\n",
    "* You model should be a Convolutional Neural Network composed of:\n",
    "  * Two convolutional layers followed by a max pooling layer. The first convolutional layer should have 32 feature maps, and the second should have 64 feature maps. Both convolutional layers should use ReLU activation, 3x3 filters, SAME padding and stride 1.\n",
    "  * One Fully Connected (FC) layer with 128 neurons, using ReLU activation.\n",
    "  * A Fully Connected output layer with 10 outputs (to classify images in the 10 classes), using Softmax activation.\n",
    "* You should apply a 25% dropout rate on the outputs of the max pooling layer, and a 50% dropout rate on the outputs of the first FC layer.\n",
    "* As usual, you should minimize the cross-entropy, using an Adam optimizer.\n",
    "* Make sure to initialize all variables using He initialization.\n",
    "* Train the model using Early Stopping.\n",
    "* Use the model to predict the class of all the images in the MNIST test set. Display all the wrong predictions it makes on the first 400 images, along with the probabilities it assigned to each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try not to peek at the solution below before you have done the exercise! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![thinking](https://upload.wikimedia.org/wikipedia/commons/0/06/Filos_segundo_logo_%28flipped%29.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10 - Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "height = 28\n",
    "width = 28\n",
    "channels = 1\n",
    "\n",
    "conv1_fmaps = 32\n",
    "conv1_ksize = 3\n",
    "conv1_stride = 1\n",
    "conv1_pad = \"SAME\"\n",
    "\n",
    "conv2_fmaps = 64\n",
    "conv2_ksize = 3\n",
    "conv2_stride = 1\n",
    "conv2_pad = \"SAME\"\n",
    "conv2_dropout_rate = 0.25\n",
    "\n",
    "pool3_fmaps = conv2_fmaps\n",
    "\n",
    "n_fc1 = 128\n",
    "fc1_dropout_rate = 0.5\n",
    "\n",
    "n_outputs = 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.name_scope(\"inputs\"):\n",
    "        X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "        X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])\n",
    "        y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "        training = tf.placeholder_with_default(False, shape=[], name='training')\n",
    "\n",
    "    conv1 = tf.layers.conv2d(X_reshaped, conv1_fmaps, kernel_size=conv1_ksize, strides=conv1_stride, padding=conv1_pad, activation=tf.nn.relu, name=\"conv1\")\n",
    "    conv2 = tf.layers.conv2d(conv1, conv2_fmaps, kernel_size=conv2_ksize, strides=conv2_stride, padding=conv2_pad, activation=tf.nn.relu, name=\"conv2\")\n",
    "\n",
    "    with tf.name_scope(\"pool3\"):\n",
    "        pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "        pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * 14 * 14])\n",
    "        pool3_flat_drop = tf.layers.dropout(pool3_flat, conv2_dropout_rate, training=training)\n",
    "\n",
    "    with tf.name_scope(\"fc1\"):\n",
    "        fc1 = tf.layers.dense(pool3_flat_drop, n_fc1, activation=tf.nn.relu, name=\"fc1\")\n",
    "        fc1_drop = tf.layers.dropout(fc1, fc1_dropout_rate, training=training)\n",
    "\n",
    "    with tf.name_scope(\"output\"):\n",
    "        logits = tf.layers.dense(fc1, n_outputs, name=\"output\")\n",
    "        Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "        loss = tf.reduce_mean(xentropy)\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    with tf.name_scope(\"init_and_save\"):\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let training begin, using early stopping. This is quite slow on a CPU, but much faster on a GPU. We achieve >99% accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 50\n",
    "\n",
    "best_acc_val = 0\n",
    "check_interval = 100\n",
    "checks_since_last_progress = 0\n",
    "max_checks_without_progress = 100\n",
    "best_model_params = None\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
    "            if iteration % check_interval == 0:\n",
    "                acc_val = accuracy.eval(feed_dict={X: mnist.test.images[:2000], y: mnist.test.labels[:2000]})\n",
    "                if acc_val > best_acc_val:\n",
    "                    best_acc_val = acc_val\n",
    "                    checks_since_last_progress = 0\n",
    "                    best_model_params = get_model_params()\n",
    "                else:\n",
    "                    checks_since_last_progress += 1\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images[2000:], y: mnist.test.labels[2000:]})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test, \"Best validation accuracy:\", best_acc_val)\n",
    "        if checks_since_last_progress > max_checks_without_progress:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "    if best_model_params:\n",
    "        restore_model_params(best_model_params)\n",
    "    acc_test = accuracy.eval(feed_dict={X: mnist.test.images[2000:], y: mnist.test.labels[2000:]})\n",
    "    print(\"Final accuracy on test set:\", acc_test)\n",
    "    save_path = saver.save(sess, \"./my_mnist_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    saver.restore(sess, \"./my_mnist_model\")\n",
    "    Y_proba_val = Y_proba.eval(feed_dict={X: mnist.test.images[2000:2400]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, y_label, y_proba in zip(mnist.test.images[2000:2400], mnist.test.labels[2000:2400], Y_proba_val):\n",
    "    y_pred = np.argmax(y_proba)\n",
    "    if y_pred != y_label:\n",
    "        print(\"Label: {}, Prediction: {}, Probabilities: {}\".format(\n",
    "            y_label, y_pred,\n",
    "            \"   \".join([\"{}={:.1f}%\".format(n, 100*p)\n",
    "                        for n, p in enumerate(y_proba) if p > 0.01])))\n",
    "        plt.imshow(image.reshape(28, 28), cmap=\"binary\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# What Next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Practice, practice and practice!\n",
    "* Go through the nice tutorials on tensorflow.org, in particular the transfer learning one.\n",
    "* Buy [my book](http://shop.oreilly.com/product/0636920052289.do)! :)  There's a lot more material, including Recurrent Neural Networks, Deep Reinforcement Learning (including the amazing DeepMind stuff), Distributed TensorFlow, Autoencoders, and much more.\n",
    "* Go through the notebooks on my other Github project [github.com/ageron/handson-ml](https://github.com/ageron/handson-ml)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![book](http://akamaicovers.oreilly.com/images/0636920052289/cat.gif)](http://shop.oreilly.com/product/0636920052289.do)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope you enjoyed this course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
